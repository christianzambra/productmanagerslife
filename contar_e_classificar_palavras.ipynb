{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIrdL7igvsBiMiRZYjCM2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianzambra/productmanagerslife/blob/main/contar_e_classificar_palavras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKaqng8JT5Nl",
        "outputId": "1cad6a64-9fb8-4890-f31f-0eb89878caf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m194.6/232.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 unidecode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAIotQ8IfPpH",
        "outputId": "55ea41b1-b3a3-463b-aa49-c16a5ff8eed4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from collections import Counter\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "import spacy"
      ],
      "metadata": {
        "id": "28jx2BfmUXQK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from spacy.lang.pt.examples import sentences\n",
        "\n",
        "# nlp = spacy.load(\"pt_core_news_sm\")\n",
        "# doc = nlp(sentences[0])\n",
        "# print(doc.text)\n",
        "# for token in doc:\n",
        "#     print(token.text, token.pos_, token.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjOUGdMfCNJ",
        "outputId": "bf0353a2-5906-4fc2-b5c6-d698104f3737"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple está querendo comprar uma startup do Reino Unido por 100 milhões de dólares\n",
            "Apple PROPN nsubj\n",
            "está AUX aux\n",
            "querendo VERB ROOT\n",
            "comprar VERB xcomp\n",
            "uma DET det\n",
            "startup NOUN obj\n",
            "do ADP case\n",
            "Reino PROPN nmod\n",
            "Unido PROPN flat:name\n",
            "por ADP case\n",
            "100 NUM obl\n",
            "milhões NUM flat\n",
            "de ADP case\n",
            "dólares NOUN nmod\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4kO81PE_dnCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_count_words(file_path, exclude_words=[]):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(file_path, 'rb') as file:\n",
        "        # Create a PDF reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to store the extracted text\n",
        "        text = ''\n",
        "\n",
        "        # Iterate through each page and extract text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    # Normalize the text to remove accents and convert to lowercase\n",
        "    normalized_text = unidecode(text.lower())\n",
        "\n",
        "    # Use regular expression to extract words (ignoring non-alphanumeric characters)\n",
        "    words = re.findall(r'\\b\\w+\\b', normalized_text)\n",
        "\n",
        "    # Exclude specified words\n",
        "    filtered_words = [word for word in words if word not in exclude_words]\n",
        "\n",
        "    # Count the occurrences of each word\n",
        "    word_counts = Counter(filtered_words)\n",
        "\n",
        "    # Get the top 5 words\n",
        "    top_words = word_counts.most_common(20)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"Total words: {len(words)}\")\n",
        "    print(\"Top 5 words:\")\n",
        "    for word, count in top_words:\n",
        "        print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "qg0cD8H7W0Lo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_and_list_top_n(file_path, exclude_words=[], top_n=5):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(file_path, 'rb') as file:\n",
        "        # Create a PDF reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to store the extracted text\n",
        "        text = ''\n",
        "\n",
        "        # Iterate through each page and extract text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    # Normalize the text to remove accents and convert to lowercase\n",
        "    normalized_text = unidecode(text.lower())\n",
        "\n",
        "    # Use regular expression to extract words (ignoring non-alphanumeric characters)\n",
        "    words = re.findall(r'\\b\\w+\\b', normalized_text)\n",
        "\n",
        "    # Exclude specified words\n",
        "    filtered_words = [word for word in words if word not in exclude_words]\n",
        "\n",
        "    # Join the filtered words into a string\n",
        "    text_for_spacy = ' '.join(filtered_words)\n",
        "\n",
        "    # Load spaCy with the Portuguese model\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text_for_spacy)\n",
        "\n",
        "    # Initialize counters for nouns and verbs\n",
        "    noun_counts = Counter()\n",
        "    verb_counts = Counter()\n",
        "\n",
        "    propn_counts = Counter()\n",
        "    num_counts = Counter()\n",
        "    adp_counts = Counter()\n",
        "    aux_counts = Counter()\n",
        "    det_counts = Counter()\n",
        "\n",
        "    # Classify words into nouns and verbs\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN':\n",
        "            noun_counts[token.text] += 1\n",
        "        elif token.pos_ == 'VERB':\n",
        "            verb_counts[token.text] += 1\n",
        "\n",
        "        elif token.pos_ == 'PROPN':\n",
        "            propn_counts[token.text] += 1\n",
        "        elif token.pos_ == 'NUM':\n",
        "            num_counts[token.text] += 1\n",
        "        elif token.pos_ == 'ADP':\n",
        "            adp_counts[token.text] += 1\n",
        "        elif token.pos_ == 'AUX':\n",
        "            aux_counts[token.text] += 1\n",
        "        elif token.pos_ == 'DET':\n",
        "            det_counts[token.text] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Get the top N most used nouns and verbs\n",
        "    top_n_nouns = [word for word, count in noun_counts.most_common(top_n)]\n",
        "    top_n_verbs = [word for word, count in verb_counts.most_common(top_n)]\n",
        "\n",
        "    top_n_propn = [word for word, count in propn_counts.most_common(top_n)]\n",
        "    top_n_num = [word for word, count in num_counts.most_common(top_n)]\n",
        "    top_n_adp = [word for word, count in adp_counts.most_common(top_n)]\n",
        "    top_n_aux = [word for word, count in aux_counts.most_common(top_n)]\n",
        "    top_n_det = [word for word, count in det_counts.most_common(top_n)]\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"In the text, the {top_n} most used nouns are: {', '.join(top_n_nouns)}\")\n",
        "    print(f\"The {top_n} most used verbs are: {', '.join(top_n_verbs)}\")\n",
        "\n",
        "    print(f\"The {top_n} most used proper name are: {', '.join(top_n_propn)}\")\n",
        "    print(f\"The {top_n} most used numbers are: {', '.join(top_n_num)}\")\n",
        "    print(f\"The {top_n} most used adp are: {', '.join(top_n_adp)}\")\n",
        "    print(f\"The {top_n} most used aux are: {', '.join(top_n_aux)}\")\n",
        "    print(f\"The {top_n} most used det are: {', '.join(top_n_det)}\")"
      ],
      "metadata": {
        "id": "it8Z2R8bdUpo"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List files in the specified directory\n",
        "directory_path = '/content/drive/MyDrive/teste_conta_palavras/'\n",
        "files = os.listdir(directory_path)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmn2PfH7YCyt",
        "outputId": "8cff5c86-9702-4bbc-91ce-5ef49a45e4b9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DomCasmurro.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Specify the correct filename in the path\n",
        "    pdf_file_path = '/content/drive/MyDrive/teste_conta_palavras/DomCasmurro.pdf'\n",
        "    exclude_words = [\n",
        "        \"e\",\n",
        "        \"a\",\n",
        "        \"que\",\n",
        "        \"de\",\n",
        "        \"o\",\n",
        "        \"nao\",\n",
        "        \"me\",\n",
        "        \"se\",\n",
        "        \"um\",\n",
        "        \"os\",\n",
        "        \"da\",\n",
        "        \"as\",\n",
        "        \"mas\",\n",
        "        \"do\",\n",
        "        \"era\",\n",
        "        \"para\",\n",
        "        \"com\",\n",
        "        \"eu\",\n",
        "        \"lhe\",\n",
        "        \"em\",\n",
        "        \"uma\",\n",
        "        \"como\",\n",
        "        \"ao\",\n",
        "        \"por\",\n",
        "        \"minha\",\n",
        "        \"no\",\n",
        "        \"mais\",\n",
        "        \"na\",\n",
        "        \"ou\",\n",
        "        \"nem\",\n",
        "        \"ele\",\n",
        "        \"mae\",\n",
        "        \"foi\",\n",
        "        \"quando\",\n",
        "        \"dias\",\n",
        "        \"tambem\",\n",
        "        \"tudo\",\n",
        "        \"ela\",\n",
        "        \"dos\",\n",
        "        \"disse\",\n",
        "        \"so\",\n",
        "        \"ser\",\n",
        "        \"meu\",\n",
        "        \"nos\",\n",
        "        \"ja\",\n",
        "        \"esta\",\n",
        "        \"mim\",\n",
        "        \"assim\",\n",
        "        \"ha\",\n",
        "        \"muito\",\n",
        "        \"nead\",\n",
        "        \"unama\"\n",
        "\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    read_pdf_count_words(pdf_file_path, exclude_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs1IbL_vUYoH",
        "outputId": "3e350fde-f63b-4dea-de92-a8d516710878"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 67467\n",
            "Top 5 words:\n",
            "capitu: 338\n",
            "casa: 170\n",
            "capitulo: 164\n",
            "olhos: 164\n",
            "jose: 159\n",
            "depois: 151\n",
            "vez: 146\n",
            "agora: 146\n",
            "das: 145\n",
            "br: 141\n",
            "la: 141\n",
            "outra: 138\n",
            "ainda: 138\n",
            "sem: 137\n",
            "voce: 136\n",
            "nada: 133\n",
            "tinha: 130\n",
            "lo: 127\n",
            "tao: 122\n",
            "mesmo: 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Specify the correct filename in the path\n",
        "    pdf_file_path = '/content/drive/MyDrive/teste_conta_palavras/DomCasmurro.pdf'\n",
        "\n",
        "    # Specify the list of words to exclude from the count\n",
        "    #exclude_words = [\"a\", \"e\", \"o\", \"lhe\"]  # Add more words as needed\n",
        "    exclude_words = [\"unad\",\"uama\",\"br\"]\n",
        "    # Specify the number of most used words to display\n",
        "    top_n = 5\n",
        "\n",
        "    classify_and_list_top_n(pdf_file_path, exclude_words, top_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GxygzXEdsdy",
        "outputId": "ff07b211-c19b-48dc-b5a0-e9169ffe94d5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the text, the 5 most used nouns are: mae, capitu, casa, olhos, vez\n",
            "The 5 most used verbs are: disse, tambem, nao, tinha, dizer\n",
            "The 5 most used proper name are: ja, dias, la, capitulo, nao\n",
            "The 5 most used numbers are: um, duas, uma, lo, www\n",
            "The 5 most used adp are: de, da, do, com, a\n",
            "The 5 most used aux are: era, foi, ser, nao, eram\n",
            "The 5 most used det are: a, o, um, os, as\n"
          ]
        }
      ]
    }
  ]
}